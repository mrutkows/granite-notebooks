{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df257e7-62fc-45cc-81cf-a2e23c25e899",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269a8125-7211-40b0-b0c9-452a8208c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a65a445f-c91f-4341-a023-baeb66118325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_object(obj):\n",
    "    for attribute in dir(obj):\n",
    "        if not attribute.startswith(\"__\") and not attribute.startswith(\"_\"): # Avoid special methods\n",
    "            try:\n",
    "                value = getattr(obj, attribute)\n",
    "                if not callable(value):                \n",
    "                    print(f\"{attribute}: {value}\")\n",
    "            except AttributeError:\n",
    "                print(f\"{attribute}: <not accessible>\") # Handle potential errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00ea6784-6378-4d46-aff3-889ba2ab49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
    "sparse_embeddings_model_path = \"ibm-granite/granite-embedding-30m-sparse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0691467c-1b74-4630-a8f6-eea914124f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)\n",
    "sparse_embeddings_tokenizer = AutoTokenizer.from_pretrained(sparse_embeddings_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3b0ec2a-cdaf-42c2-ab30-24249370ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config = AutoConfig.from_pretrained(embeddings_model_path)\n",
    "sparse_config = AutoConfig.from_pretrained(sparse_embeddings_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de445c03-471f-4a8a-9e0f-76f5184c143f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_cross_attention: False\n",
      "architectures: ['RobertaModel']\n",
      "attention_probs_dropout_prob: 0.1\n",
      "attribute_map: {}\n",
      "bad_words_ids: None\n",
      "base_config_key: \n",
      "base_model_pp_plan: None\n",
      "base_model_tp_plan: None\n",
      "begin_suppress_tokens: None\n",
      "bos_token_id: 0\n",
      "chunk_size_feed_forward: 0\n",
      "classifier_dropout: None\n",
      "cross_attention_hidden_size: None\n",
      "decoder_start_token_id: None\n",
      "diversity_penalty: 0.0\n",
      "do_sample: False\n",
      "early_stopping: False\n",
      "encoder_no_repeat_ngram_size: 0\n",
      "eos_token_id: 2\n",
      "exponential_decay_length_penalty: None\n",
      "finetuning_task: None\n",
      "forced_bos_token_id: None\n",
      "forced_eos_token_id: None\n",
      "hidden_act: gelu\n",
      "hidden_dropout_prob: 0.1\n",
      "hidden_size: 384\n",
      "id2label: {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "initializer_range: 0.02\n",
      "intermediate_size: 1536\n",
      "is_composition: False\n",
      "is_decoder: False\n",
      "is_encoder_decoder: False\n",
      "label2id: {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "layer_norm_eps: 1e-12\n",
      "length_penalty: 1.0\n",
      "max_length: 20\n",
      "max_position_embeddings: 514\n",
      "min_length: 0\n",
      "model_type: roberta\n",
      "name_or_path: ibm-granite/granite-embedding-30m-english\n",
      "no_repeat_ngram_size: 0\n",
      "num_attention_heads: 12\n",
      "num_beam_groups: 1\n",
      "num_beams: 1\n",
      "num_hidden_layers: 6\n",
      "num_labels: 2\n",
      "num_return_sequences: 1\n",
      "output_attentions: False\n",
      "output_hidden_states: False\n",
      "output_scores: False\n",
      "pad_token_id: 1\n",
      "position_embedding_type: absolute\n",
      "prefix: None\n",
      "problem_type: None\n",
      "pruned_heads: {}\n",
      "remove_invalid_values: False\n",
      "repetition_penalty: 1.0\n",
      "return_dict: True\n",
      "return_dict_in_generate: False\n",
      "sep_token_id: None\n",
      "sub_configs: {}\n",
      "suppress_tokens: None\n",
      "task_specific_params: None\n",
      "temperature: 1.0\n",
      "tf_legacy_loss: False\n",
      "tie_encoder_decoder: False\n",
      "tie_word_embeddings: True\n",
      "tokenizer_class: None\n",
      "top_k: 50\n",
      "top_p: 1.0\n",
      "torch_dtype: torch.bfloat16\n",
      "torchscript: False\n",
      "transformers_version: 4.15.0\n",
      "type_vocab_size: 2\n",
      "typical_p: 1.0\n",
      "use_bfloat16: False\n",
      "use_cache: True\n",
      "use_return_dict: True\n",
      "vocab_size: 50265\n"
     ]
    }
   ],
   "source": [
    "dump_object(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8940a2ec-6bfc-42ef-9ba1-deaebb852ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_cross_attention: False\n",
      "architectures: ['RobertaForMaskedLM']\n",
      "attention_probs_dropout_prob: 0.1\n",
      "attribute_map: {}\n",
      "bad_words_ids: None\n",
      "base_config_key: \n",
      "base_model_pp_plan: None\n",
      "base_model_tp_plan: None\n",
      "begin_suppress_tokens: None\n",
      "bos_token_id: 0\n",
      "chunk_size_feed_forward: 0\n",
      "classifier_dropout: None\n",
      "cross_attention_hidden_size: None\n",
      "decoder_start_token_id: None\n",
      "diversity_penalty: 0.0\n",
      "do_sample: False\n",
      "early_stopping: False\n",
      "encoder_no_repeat_ngram_size: 0\n",
      "eos_token_id: 2\n",
      "exponential_decay_length_penalty: None\n",
      "finetuning_task: None\n",
      "forced_bos_token_id: None\n",
      "forced_eos_token_id: None\n",
      "hidden_act: gelu\n",
      "hidden_dropout_prob: 0.1\n",
      "hidden_size: 384\n",
      "id2label: {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "initializer_range: 0.02\n",
      "intermediate_size: 1536\n",
      "is_composition: False\n",
      "is_decoder: False\n",
      "is_encoder_decoder: False\n",
      "label2id: {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "layer_norm_eps: 1e-12\n",
      "length_penalty: 1.0\n",
      "max_length: 20\n",
      "max_position_embeddings: 514\n",
      "min_length: 0\n",
      "model_type: roberta\n",
      "name_or_path: ibm-granite/granite-embedding-30m-sparse\n",
      "no_repeat_ngram_size: 0\n",
      "num_attention_heads: 12\n",
      "num_beam_groups: 1\n",
      "num_beams: 1\n",
      "num_hidden_layers: 6\n",
      "num_labels: 2\n",
      "num_return_sequences: 1\n",
      "output_attentions: False\n",
      "output_hidden_states: False\n",
      "output_scores: False\n",
      "pad_token_id: 1\n",
      "position_embedding_type: absolute\n",
      "prefix: None\n",
      "problem_type: None\n",
      "pruned_heads: {}\n",
      "remove_invalid_values: False\n",
      "repetition_penalty: 1.0\n",
      "return_dict: True\n",
      "return_dict_in_generate: False\n",
      "sep_token_id: None\n",
      "sub_configs: {}\n",
      "suppress_tokens: None\n",
      "task_specific_params: None\n",
      "temperature: 1.0\n",
      "tf_legacy_loss: False\n",
      "tie_encoder_decoder: False\n",
      "tie_word_embeddings: True\n",
      "tokenizer_class: None\n",
      "top_k: 50\n",
      "top_p: 1.0\n",
      "torch_dtype: torch.bfloat16\n",
      "torchscript: False\n",
      "transformers_version: 4.48.2\n",
      "type_vocab_size: 2\n",
      "typical_p: 1.0\n",
      "use_bfloat16: False\n",
      "use_cache: True\n",
      "use_return_dict: True\n",
      "vocab_size: 50265\n"
     ]
    }
   ],
   "source": [
    "dump_object(sparse_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "641feec5-82fc-4d6b-82bf-1286b5bc9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(embeddings_model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2039a76-7a96-4f05-bfc3-a940a8f3c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model = AutoModelForMaskedLM.from_pretrained(sparse_embeddings_model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a38b9c83-1345-42e3-b41a-5e2c4a0be713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   0,  713,   16,   10, 7728, 3645,    4,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample sentence.\"\n",
    "tokenized_input = sparse_embeddings_tokenizer(text, return_tensors=\"pt\") #  or tokenizer(text)\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f44ad3f-8089-4903-8981-5c765b02993b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0917,  0.6270,  0.2908,  ...,  0.8090, -7.7908,  0.4029],\n",
       "         [-0.9681,  0.4096,  1.1218,  ...,  1.9385, -8.3621, -1.2195],\n",
       "         [-0.6250,  0.4209,  1.2987,  ...,  2.0155, -8.8782, -1.3207],\n",
       "         ...,\n",
       "         [-0.7753,  0.2901,  1.3131,  ...,  1.8260, -7.9409, -1.2732],\n",
       "         [-0.7346,  0.1128,  0.9503,  ...,  1.8745, -8.2791, -1.2013],\n",
       "         [-1.1832,  0.1630,  0.9972,  ...,  1.6418, -8.7131, -1.3595]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.0740, -0.3853, -0.3833,  0.8431,  0.8284, -0.5442,  0.0567, -0.7814,\n",
       "         -0.0306, -0.7020, -0.5249,  0.8708,  0.2952,  0.2342, -0.1667, -0.3459,\n",
       "          0.9935,  0.2066,  0.4474, -0.2957,  0.6336, -0.9835, -0.0151,  0.6406,\n",
       "          0.3330, -0.7784,  0.4974, -0.1368,  0.8830,  0.9333, -0.8930, -0.5642,\n",
       "         -0.9814, -0.4812, -0.2500,  0.5129,  0.2521,  0.2317,  0.2613, -0.5998,\n",
       "         -0.8011,  0.3829,  0.3943,  0.6421, -0.0899,  0.7475, -0.1785, -0.0082,\n",
       "         -0.2771,  0.1523,  0.6424,  0.6257,  0.7996, -0.1977,  0.5179,  0.1304,\n",
       "          0.9779, -0.2864, -0.1211,  0.9259, -0.0719,  0.8690,  0.7548, -0.1245,\n",
       "          0.4945, -0.6909, -0.7082,  0.7972,  0.3037, -0.1003, -0.9856, -0.7336,\n",
       "          0.9263,  0.9256, -0.8884, -0.9517,  0.9198, -0.2218, -0.6007, -0.2623,\n",
       "          0.0354, -0.2350, -0.6265, -0.4618,  0.0776, -0.6907, -0.3646, -0.6589,\n",
       "         -0.9870,  0.1958, -0.0291, -0.1279, -0.4441,  0.9904, -0.3208,  0.8392,\n",
       "          0.8351,  0.9643, -0.4190, -0.9130, -0.6051, -0.1850,  0.8151,  0.6643,\n",
       "          0.2722,  0.4669,  0.3551, -0.7673,  0.7879,  0.7449,  0.0866, -0.6874,\n",
       "         -0.7979,  0.1803, -0.9675,  0.4199,  0.5971, -0.4924, -0.8776,  0.7409,\n",
       "         -0.2957, -0.6598, -0.2154,  0.8744,  0.4743,  0.4758, -0.7111,  0.7325,\n",
       "         -0.0569, -0.9180, -0.7966, -0.7318,  0.6431,  0.6225,  0.1282,  0.9302,\n",
       "          0.7044,  0.7727, -0.9225, -0.5873,  0.2262, -0.6574, -0.7665,  0.2256,\n",
       "         -0.2621, -0.1845, -0.7267, -0.2174, -0.8375, -0.4397, -0.0468, -0.2357,\n",
       "         -0.4930,  0.8122, -0.0078,  0.1542, -0.1248,  0.8483,  0.3772,  0.4835,\n",
       "         -0.1310, -0.7915, -0.4309,  0.6175, -0.6895,  0.8280, -0.5300, -0.4154,\n",
       "          0.8591, -0.8797, -0.0159,  0.8889,  0.1511,  0.4267,  0.9700,  0.1359,\n",
       "         -0.6698,  0.9232,  0.3406, -0.7900, -0.4410,  0.5936,  0.9201, -0.7189,\n",
       "         -0.1394,  0.1477, -0.9965,  0.9823,  0.1406, -0.2418,  0.5606,  0.5792,\n",
       "         -0.8042, -0.4254,  0.6052, -0.4595, -0.2502, -0.9520,  0.2197, -0.4823,\n",
       "          0.5170,  0.8091, -0.3879, -0.4639, -0.8260, -0.4458,  0.9609, -0.0692,\n",
       "         -0.0043,  0.1441,  0.5697, -0.8067, -0.2069, -0.4326,  0.8798,  0.1013,\n",
       "          0.1984,  0.6573,  0.1543, -0.2530,  0.6836,  0.5762, -0.5490, -0.5002,\n",
       "          0.2256,  0.8929,  0.4125, -0.9108, -0.8501,  0.8949, -0.9295,  0.7267,\n",
       "          0.6709, -0.2730, -0.9782,  0.2977,  0.0281, -0.6427,  0.6617,  0.3264,\n",
       "          0.6242,  0.0778,  0.4898,  0.4216,  0.0653, -0.7917,  0.7115, -0.8953,\n",
       "         -0.1242,  0.6717,  0.8041, -0.2524,  0.8982,  0.0334, -0.7311,  0.8276,\n",
       "         -0.4789, -0.1192,  0.6921,  0.3642,  0.9173, -0.3858,  0.2192,  0.6478,\n",
       "         -0.9039, -0.9603,  0.3174,  0.4122,  0.0102, -0.9031, -0.4773, -0.9203,\n",
       "         -0.9711,  0.2491,  0.5398,  0.1465,  0.7731, -0.3636,  0.0856, -0.9249,\n",
       "         -0.9402, -0.8143,  0.9617, -0.9236,  0.4501,  0.6816, -0.6817, -0.4293,\n",
       "         -0.7997,  0.4914, -0.4586, -0.7421,  0.8378,  0.5533,  0.4648,  0.7744,\n",
       "         -0.2198, -0.6546, -0.6248, -0.8976, -0.7265,  0.2899,  0.2204,  0.4683,\n",
       "         -0.8662,  0.9477,  0.9595, -0.7102,  0.0213, -0.2854, -0.7403, -0.5611,\n",
       "          0.4109,  0.5524, -0.7081, -0.7135,  0.7712,  0.4315,  0.1582, -0.7147,\n",
       "         -0.8564,  0.7984, -0.9056, -0.2025, -0.7665, -0.9329,  0.1605,  0.6742,\n",
       "         -0.8143, -0.2203, -0.5210,  0.4447, -0.9348, -0.5459,  0.9094, -0.5407,\n",
       "          0.0126, -0.3886, -0.4366,  0.2024, -0.6587,  0.5162, -0.6502, -0.9549,\n",
       "         -0.2265, -0.4854, -0.9864,  0.7529, -0.3086,  0.6014, -0.1915, -0.0193,\n",
       "         -0.9411,  0.8741,  0.9820, -0.9875, -0.7178, -0.0633, -0.8672,  0.4341,\n",
       "          0.3395,  0.9306,  0.3395,  0.3136,  0.5801,  0.7918,  0.9851, -0.2711,\n",
       "         -0.9076,  0.7507, -0.0567, -0.8593,  0.6922,  0.9583,  0.1962,  0.8044,\n",
       "          0.2990, -0.5986,  0.3657,  0.9466,  0.9195, -0.5041,  0.3108, -0.8859]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**tokenized_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e97f884-8782-4e9d-9c1f-592398d82be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daef3e66-bac7-4a86-a467-b0f61e25153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attentions: None\n",
      "cross_attentions: None\n",
      "hidden_states: None\n",
      "last_hidden_state: tensor([[[ 0.0917,  0.6270,  0.2908,  ...,  0.8090, -7.7908,  0.4029],\n",
      "         [-0.9681,  0.4096,  1.1218,  ...,  1.9385, -8.3621, -1.2195],\n",
      "         [-0.6250,  0.4209,  1.2987,  ...,  2.0155, -8.8782, -1.3207],\n",
      "         ...,\n",
      "         [-0.7753,  0.2901,  1.3131,  ...,  1.8260, -7.9409, -1.2732],\n",
      "         [-0.7346,  0.1128,  0.9503,  ...,  1.8745, -8.2791, -1.2013],\n",
      "         [-1.1832,  0.1630,  0.9972,  ...,  1.6418, -8.7131, -1.3595]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "past_key_values: None\n",
      "pooler_output: tensor([[ 0.0740, -0.3853, -0.3833,  0.8431,  0.8284, -0.5442,  0.0567, -0.7814,\n",
      "         -0.0306, -0.7020, -0.5249,  0.8708,  0.2952,  0.2342, -0.1667, -0.3459,\n",
      "          0.9935,  0.2066,  0.4474, -0.2957,  0.6336, -0.9835, -0.0151,  0.6406,\n",
      "          0.3330, -0.7784,  0.4974, -0.1368,  0.8830,  0.9333, -0.8930, -0.5642,\n",
      "         -0.9814, -0.4812, -0.2500,  0.5129,  0.2521,  0.2317,  0.2613, -0.5998,\n",
      "         -0.8011,  0.3829,  0.3943,  0.6421, -0.0899,  0.7475, -0.1785, -0.0082,\n",
      "         -0.2771,  0.1523,  0.6424,  0.6257,  0.7996, -0.1977,  0.5179,  0.1304,\n",
      "          0.9779, -0.2864, -0.1211,  0.9259, -0.0719,  0.8690,  0.7548, -0.1245,\n",
      "          0.4945, -0.6909, -0.7082,  0.7972,  0.3037, -0.1003, -0.9856, -0.7336,\n",
      "          0.9263,  0.9256, -0.8884, -0.9517,  0.9198, -0.2218, -0.6007, -0.2623,\n",
      "          0.0354, -0.2350, -0.6265, -0.4618,  0.0776, -0.6907, -0.3646, -0.6589,\n",
      "         -0.9870,  0.1958, -0.0291, -0.1279, -0.4441,  0.9904, -0.3208,  0.8392,\n",
      "          0.8351,  0.9643, -0.4190, -0.9130, -0.6051, -0.1850,  0.8151,  0.6643,\n",
      "          0.2722,  0.4669,  0.3551, -0.7673,  0.7879,  0.7449,  0.0866, -0.6874,\n",
      "         -0.7979,  0.1803, -0.9675,  0.4199,  0.5971, -0.4924, -0.8776,  0.7409,\n",
      "         -0.2957, -0.6598, -0.2154,  0.8744,  0.4743,  0.4758, -0.7111,  0.7325,\n",
      "         -0.0569, -0.9180, -0.7966, -0.7318,  0.6431,  0.6225,  0.1282,  0.9302,\n",
      "          0.7044,  0.7727, -0.9225, -0.5873,  0.2262, -0.6574, -0.7665,  0.2256,\n",
      "         -0.2621, -0.1845, -0.7267, -0.2174, -0.8375, -0.4397, -0.0468, -0.2357,\n",
      "         -0.4930,  0.8122, -0.0078,  0.1542, -0.1248,  0.8483,  0.3772,  0.4835,\n",
      "         -0.1310, -0.7915, -0.4309,  0.6175, -0.6895,  0.8280, -0.5300, -0.4154,\n",
      "          0.8591, -0.8797, -0.0159,  0.8889,  0.1511,  0.4267,  0.9700,  0.1359,\n",
      "         -0.6698,  0.9232,  0.3406, -0.7900, -0.4410,  0.5936,  0.9201, -0.7189,\n",
      "         -0.1394,  0.1477, -0.9965,  0.9823,  0.1406, -0.2418,  0.5606,  0.5792,\n",
      "         -0.8042, -0.4254,  0.6052, -0.4595, -0.2502, -0.9520,  0.2197, -0.4823,\n",
      "          0.5170,  0.8091, -0.3879, -0.4639, -0.8260, -0.4458,  0.9609, -0.0692,\n",
      "         -0.0043,  0.1441,  0.5697, -0.8067, -0.2069, -0.4326,  0.8798,  0.1013,\n",
      "          0.1984,  0.6573,  0.1543, -0.2530,  0.6836,  0.5762, -0.5490, -0.5002,\n",
      "          0.2256,  0.8929,  0.4125, -0.9108, -0.8501,  0.8949, -0.9295,  0.7267,\n",
      "          0.6709, -0.2730, -0.9782,  0.2977,  0.0281, -0.6427,  0.6617,  0.3264,\n",
      "          0.6242,  0.0778,  0.4898,  0.4216,  0.0653, -0.7917,  0.7115, -0.8953,\n",
      "         -0.1242,  0.6717,  0.8041, -0.2524,  0.8982,  0.0334, -0.7311,  0.8276,\n",
      "         -0.4789, -0.1192,  0.6921,  0.3642,  0.9173, -0.3858,  0.2192,  0.6478,\n",
      "         -0.9039, -0.9603,  0.3174,  0.4122,  0.0102, -0.9031, -0.4773, -0.9203,\n",
      "         -0.9711,  0.2491,  0.5398,  0.1465,  0.7731, -0.3636,  0.0856, -0.9249,\n",
      "         -0.9402, -0.8143,  0.9617, -0.9236,  0.4501,  0.6816, -0.6817, -0.4293,\n",
      "         -0.7997,  0.4914, -0.4586, -0.7421,  0.8378,  0.5533,  0.4648,  0.7744,\n",
      "         -0.2198, -0.6546, -0.6248, -0.8976, -0.7265,  0.2899,  0.2204,  0.4683,\n",
      "         -0.8662,  0.9477,  0.9595, -0.7102,  0.0213, -0.2854, -0.7403, -0.5611,\n",
      "          0.4109,  0.5524, -0.7081, -0.7135,  0.7712,  0.4315,  0.1582, -0.7147,\n",
      "         -0.8564,  0.7984, -0.9056, -0.2025, -0.7665, -0.9329,  0.1605,  0.6742,\n",
      "         -0.8143, -0.2203, -0.5210,  0.4447, -0.9348, -0.5459,  0.9094, -0.5407,\n",
      "          0.0126, -0.3886, -0.4366,  0.2024, -0.6587,  0.5162, -0.6502, -0.9549,\n",
      "         -0.2265, -0.4854, -0.9864,  0.7529, -0.3086,  0.6014, -0.1915, -0.0193,\n",
      "         -0.9411,  0.8741,  0.9820, -0.9875, -0.7178, -0.0633, -0.8672,  0.4341,\n",
      "          0.3395,  0.9306,  0.3395,  0.3136,  0.5801,  0.7918,  0.9851, -0.2711,\n",
      "         -0.9076,  0.7507, -0.0567, -0.8593,  0.6922,  0.9583,  0.1962,  0.8044,\n",
      "          0.2990, -0.5986,  0.3657,  0.9466,  0.9195, -0.5041,  0.3108, -0.8859]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dump_object(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158b8c5-5289-429f-8e54-3c5276749ba7",
   "metadata": {},
   "source": [
    "from HF Transformers: [transformers/src/transformers modeling_outputs.py](https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L70)\n",
    "```\n",
    "class BaseModelOutputWithPooling(ModelOutput):\n",
    "...\n",
    "last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
    "    Sequence of hidden-states at the output of the last layer of the model.\n",
    "pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n",
    "    Last layer hidden-state of the first token of the sequence (classification token) after further processing\n",
    "    through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns\n",
    "    the classification token after processing through a linear layer and a tanh activation function (The tanh\n",
    "    function outputs values in the range of -1 to +1). \n",
    "    The linear layer weights are trained from the next sentence prediction (classification) objective during\n",
    "    pretraining.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab769b1-fab9-47c9-b8e8-62b247c48a1d",
   "metadata": {},
   "source": [
    "Understanding shapes from PyTorch: \n",
    "- [Tensor Shapes](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html#tensor-shapes)\n",
    "- [torch.Tensor.shape](https://pytorch.org/docs/stable/generated/torch.Tensor.shape.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a13d8780-5785-4836-a9c1-ba0469a463d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 384])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ([rows, num_rows (matches input tokens), columns (sequence_length)])\n",
    "# contains: logits (un-normalized values)\n",
    "output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65121892-f669-4454-a746-136966e203c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ([rows, sequence_length])\n",
    "# containst: embeddings (normlized, using tanh fx.)\n",
    "output.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95bde0-d49f-42c2-9c93-7c1b921eb806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
