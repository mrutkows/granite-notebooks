{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granite Sparse Demo\n",
    "- See: https://github.com/primeqa/docuverse/blob/v0.0.12/notebooks/GraniteSparseTest.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest and Retriece using PyMilvus\n",
    "\n",
    "`pip install pymilvus[model]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see a bit more of the tensors...\n",
    "torch.set_printoptions(threshold=10_000, linewidth=200, edgeitems=4) # edgeitems=3, precision=4, sci_mode=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_object(obj):\n",
    "    for attribute in dir(obj):\n",
    "        if not attribute.startswith(\"__\") and not attribute.startswith(\"_\"): # Avoid special methods\n",
    "            try:\n",
    "                value = getattr(obj, attribute)\n",
    "                if not callable(value):                \n",
    "                    print(f\"{attribute}: {value}\")\n",
    "            except AttributeError:\n",
    "                print(f\"{attribute}: <not accessible>\") # Handle potential errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveNumpy3D(numpy_array, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "       for i in range(numpy_array.shape[0]):\n",
    "           np.savetxt(file, numpy_array[i, :, :], fmt='%f')\n",
    "           file.write(\"\\n\")  # Add a newline between slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import model\n",
    "from pymilvus import MilvusClient, DataType\n",
    "\n",
    "client = MilvusClient(\"./milvus_demo.db\")\n",
    "client.drop_collection(collection_name=\"my_sparse_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auto_id': True, 'description': '', 'fields': [{'name': 'pk', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 100}, 'is_primary': True, 'auto_id': False}], 'enable_dynamic_field': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = client.create_schema(\n",
    "    auto_id=True,\n",
    "    enable_dynamic_fields=True,\n",
    ")\n",
    "schema.add_field(field_name=\"pk\", datatype=DataType.VARCHAR, is_primary=True, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auto_id': True, 'description': '', 'fields': [{'name': 'pk', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 100}, 'is_primary': True, 'auto_id': False}, {'name': 'id', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 100}}], 'enable_dynamic_field': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.add_field(field_name=\"id\", datatype=DataType.VARCHAR, is_primary=False, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auto_id': True, 'description': '', 'fields': [{'name': 'pk', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 100}, 'is_primary': True, 'auto_id': False}, {'name': 'id', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 100}}, {'name': 'embeddings', 'description': '', 'type': <DataType.SPARSE_FLOAT_VECTOR: 104>}], 'enable_dynamic_field': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.add_field(field_name=\"embeddings\", datatype=DataType.SPARSE_FLOAT_VECTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = client.prepare_index_params()\n",
    "index_params.add_index(field_name=\"embeddings\",\n",
    "                               index_name=\"sparse_inverted_index\",\n",
    "                               index_type=\"SPARSE_INVERTED_INDEX\",\n",
    "                               metric_type=\"IP\",\n",
    "                               params={\"drop_ratio_build\": 0.2})\n",
    "client.create_collection(\n",
    "    collection_name=\"my_sparse_collection\",\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = model.sparse.SpladeEmbeddingFunction(\n",
    "    model_name=\"ibm-granite/granite-embedding-30m-sparse\", \n",
    "    device=\"cpu\",\n",
    "    batch_size=2,\n",
    "    k_tokens_query=50,\n",
    "    k_tokens_document=192\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare documents to be ingested\n",
    "docs = [\n",
    "    \"Artificial intelligence was founded as an academic discipline in 1956.\",\n",
    "    \"Alan Turing was the first person to conduct substantial research in AI.\",\n",
    "    \"Born in Maida Vale, London, Turing was raised in southern England.\",\n",
    "]\n",
    "\n",
    "# SpladeEmbeddingFunction.encode_documents returns sparse matrix or sparse array depending\n",
    "# on the milvus-model version. reshape(1,-1) ensures the format is correct for ingestion.\n",
    "doc_vector = [{\"embeddings\": doc_emb.reshape(1,-1), \"id\": f\"item_{i}\"} for i, doc_emb in enumerate(embeddings_model.encode_documents(docs))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'list'>\n",
      "[{'embeddings': <COOrdinate sparse array of dtype 'float32'\n",
      "\twith 192 stored elements and shape (1, 50265)>, 'id': 'item_0'}, {'embeddings': <COOrdinate sparse array of dtype 'float32'\n",
      "\twith 192 stored elements and shape (1, 50265)>, 'id': 'item_1'}, {'embeddings': <COOrdinate sparse array of dtype 'float32'\n",
      "\twith 192 stored elements and shape (1, 50265)>, 'id': 'item_2'}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"type: {type(doc_vector)}\\n{doc_vector}\")\n",
    "dump_object(doc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embeddings': <COOrdinate sparse array of dtype 'float32'\n",
      "\twith 192 stored elements and shape (1, 50265)>, 'id': 'item_0'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{doc_vector[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '456922559366037504', 'distance': 12.364128112792969, 'entity': {'id': 'item_0'}}]\n",
      "[{'id': '456922559366037506', 'distance': 17.135875701904297, 'entity': {'id': 'item_2'}}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client.insert(\n",
    "    collection_name=\"my_sparse_collection\",\n",
    "    data=doc_vector\n",
    ")\n",
    "\n",
    "# Prepare search parameters\n",
    "search_params = {\n",
    "    \"params\": {\"drop_ratio_search\": 0.2},  # Additional optional search parameters\n",
    "}\n",
    "\n",
    "# Prepare the query vector\n",
    "\n",
    "queries = [\n",
    "      \"When was artificial intelligence founded\", \n",
    "      \"Where was Turing born?\"\n",
    "]\n",
    "query_vector = embeddings_model.encode_documents(queries)\n",
    "\n",
    "res = client.search(\n",
    "    collection_name=\"my_sparse_collection\",\n",
    "    data=query_vector,\n",
    "    limit=1, #top k documents to return\n",
    "    output_fields=[\"id\"],\n",
    "    search_params=search_params,\n",
    ")\n",
    "\n",
    "for r in res:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Embeddings using HF Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> torch.log() function calculates the natural logarithm (base e) of each element in the input tensor, returning a new tensor with the log values\n",
    "\n",
    "```python\n",
    "import torch\n",
    "a = torch.rand(5) * 5\n",
    "print(a)\n",
    "# Output: tensor([4.7767, 4.3234, 1.2156, 0.2411, 4.5739])\n",
    "b = torch.log(a)\n",
    "print(b)\n",
    "# Output: tensor([ 1.5637, 1.4640, 0.1952, -1.4226, 1.5204])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseSentenceTransformer:\n",
    "    def __init__(self, model_name_or_path, device:str= 'cpu'):\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name_or_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        if device == \"cuda\":\n",
    "            self.model = self.model.cuda()\n",
    "            self.model = self.model.bfloat16()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, sentences, max_tokens=20):        \n",
    "        if type(sentences) == str:\n",
    "            sentences = [sentences]\n",
    "        \n",
    "        input_dict = self.tokenizer(sentences, max_length=512, padding=True, return_tensors='pt', truncation=True)\n",
    "        attention_mask = input_dict['attention_mask']  # (bs,seqlen)\n",
    "        print(f\"---\\nattention_mask: shape: {attention_mask.shape}:\\n{attention_mask}\")\n",
    "\n",
    "        if self.device == \"cuda\":\n",
    "            input_dict['input_ids'] = input_dict['input_ids'].cuda()\n",
    "            input_dict['attention_mask'] = input_dict['attention_mask'].cuda()\n",
    "            if 'token_type_ids' in input_dict:\n",
    "                input_dict['token_type_ids'] = input_dict['token_type_ids'].cuda()\n",
    "\n",
    "        hidden_state = self.model(**input_dict)[0]\n",
    "        print(f\"---\\nhidden_state: shape: {hidden_state.shape}:\\n{hidden_state}\")\n",
    "        \n",
    "        # Note we add 1.0 to avoid (default) of (-)Infty being set as values\n",
    "        maxarg = torch.log(1.0 + torch.relu(hidden_state))\n",
    "        print(f\"---\\nmaxarg: shape: {maxarg.shape}:\\n{maxarg}\")\n",
    "        saveNumpy3D(maxarg, \"maxarg.txt\")\n",
    "        \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).to(maxarg.device) # bs * seqlen * voc\n",
    "        print(f\"---\\ninput_mask_expanded: shape: {input_mask_expanded.shape}:\\n{input_mask_expanded}\")   \n",
    "        saveNumpy3D(input_mask_expanded, \"input_mask_expanded.txt\")\n",
    "        \n",
    "        maxdim1 = torch.max(maxarg * input_mask_expanded, dim=1).values  # bs * voc\n",
    "        print(f\"---\\nmaxdim1: shape: {maxdim1.shape}:\\n{maxdim1}\")\n",
    "        # saveNumpy3D(maxdim1, \"maxdim1.txt\")\n",
    "        np.savetxt(\"maxdim1.txt\", maxdim1, fmt='%f')        \n",
    "        \n",
    "        # get topk high weights\n",
    "        topk, indices = torch.topk(maxdim1, k=max_tokens) # (weight - (bs * max_terms), index - (bs * max_terms))\n",
    "        print (f\"---\\ntopk.shape: {topk.shape}:\\n{topk}\")\n",
    "\n",
    "        expansions = [[(self.tokenizer.decode(int(indices[sidx][tidx])), float(topk[sidx][tidx])) for tidx in range(topk.shape[1])] for sidx in range(topk.shape[0]) ]  \n",
    "\n",
    "        return expansions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model = SparseSentenceTransformer(\"ibm-granite/granite-embedding-30m-sparse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "attention_mask: shape: torch.Size([1, 14]):\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "---\n",
      "hidden_state: shape: torch.Size([1, 14, 50265]):\n",
      "tensor([[[ -1.7427,  -7.6422,  -1.9149,  -8.9616,  ..., -10.7275, -11.0216, -11.6262, -10.0269],\n",
      "         [ -2.0706,  -7.3661,  -1.9145,  -7.8359,  ...,  -9.7414,  -9.9994, -10.6886,  -8.8855],\n",
      "         [ -2.7542,  -8.0620,  -2.3382, -10.5351,  ..., -12.5394, -13.0191, -13.6108, -11.1574],\n",
      "         [ -2.4148,  -7.8324,  -1.5616, -10.6918,  ..., -12.1566, -12.6419, -13.1174, -11.0873],\n",
      "         ...,\n",
      "         [ -1.5354,  -7.3953,  -2.3199,  -8.7108,  ..., -10.0220, -10.3128, -11.0252,  -9.5884],\n",
      "         [ -3.2807,  -7.0404,  -3.2199,  -8.2233,  ...,  -9.7368, -10.0086, -10.7926, -10.0631],\n",
      "         [ -1.4550,  -7.6613,  -2.2228,  -8.3820,  ...,  -9.8845, -10.1485, -10.7153,  -8.8985],\n",
      "         [ -1.5066,  -7.5331,  -2.0919,  -7.9797,  ...,  -9.3315,  -9.5452, -10.1722,  -8.3641]]])\n",
      "---\n",
      "maxarg: shape: torch.Size([1, 14, 50265]):\n",
      "tensor([[[0., 0., 0., 0.,  ..., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.,  ..., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.,  ..., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.,  ..., 0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.,  ..., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.,  ..., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.,  ..., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.,  ..., 0., 0., 0., 0.]]])\n",
      "---\n",
      "input_mask_expanded: shape: torch.Size([1, 14, 1]):\n",
      "tensor([[[1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [1]]])\n",
      "---\n",
      "maxdim1: shape: torch.Size([1, 50265]):\n",
      "tensor([[0., 0., 0., 0.,  ..., 0., 0., 0., 0.]])\n",
      "---\n",
      "topk.shape: torch.Size([1, 40]):\n",
      "tensor([[1.6672, 1.4905, 1.2501, 1.2193, 1.0604, 1.0351, 0.9786, 0.7224, 0.6999, 0.6893, 0.6567, 0.6217, 0.5886, 0.5614, 0.5508, 0.5432, 0.5026, 0.4764, 0.4488, 0.4465, 0.4328, 0.4083, 0.3749, 0.3723,\n",
      "         0.3303, 0.3248, 0.3157, 0.2725, 0.2713, 0.2222, 0.2132, 0.1875, 0.1113, 0.0789, 0.0599, 0.0512, 0.0400, 0.0390, 0.0357, 0.0210]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(' AI', 1.6671510934829712),\n",
       "  (' intelligence', 1.4905357360839844),\n",
       "  (' artificial', 1.2501304149627686),\n",
       "  (' discipline', 1.2192901372909546),\n",
       "  (' founded', 1.060374140739441),\n",
       "  (' 1956', 1.0351001024246216),\n",
       "  (' invention', 0.9785776138305664),\n",
       "  ('56', 0.7224239706993103),\n",
       "  (' learning', 0.6999132037162781),\n",
       "  (' scientific', 0.6892709732055664),\n",
       "  (' computer', 0.6566586494445801),\n",
       "  (' academic', 0.62173992395401),\n",
       "  (' university', 0.5886272192001343),\n",
       "  (' robot', 0.5613628625869751),\n",
       "  (' establishment', 0.5508407950401306),\n",
       "  (' philosophy', 0.5431841611862183),\n",
       "  ('A', 0.5025954842567444),\n",
       "  (' brain', 0.47637760639190674),\n",
       "  (' machine', 0.44881123304367065),\n",
       "  ('1960', 0.446492463350296),\n",
       "  ('1950', 0.4327620565891266),\n",
       "  (' algorithm', 0.4083283841609955),\n",
       "  (' science', 0.37494218349456787),\n",
       "  (' regression', 0.3722963333129883),\n",
       "  (' comput', 0.33030468225479126),\n",
       "  (' Discipline', 0.32480597496032715),\n",
       "  (' institute', 0.3156873881816864),\n",
       "  (' automatic', 0.27251482009887695),\n",
       "  (' technology', 0.27132272720336914),\n",
       "  (' philosopher', 0.22217071056365967),\n",
       "  (' classification', 0.21323901414871216),\n",
       "  (' Evolution', 0.18753837049007416),\n",
       "  (' publication', 0.11127319931983948),\n",
       "  (' India', 0.07885350286960602),\n",
       "  ('history', 0.05991281569004059),\n",
       "  (' Art', 0.0512155257165432),\n",
       "  (' engineering', 0.040040019899606705),\n",
       "  (' PhD', 0.03902957960963249),\n",
       "  (' developed', 0.03565627709031105),\n",
       "  (' Robot', 0.02100963331758976)]]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change max_tokens to produce more or less expansions for the sentences\n",
    "sparse_model.encode([\"Artificial intelligence was founded as an academic discipline in 1956.\"], max_tokens=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
